import sys, os
sys.stdout.reconfigure(encoding='utf-8')
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
from tqdm import tqdm

mp_pose = mp.solutions.pose

# âœ… ìš°ë¦¬ê°€ í•™ìŠµí•  7ê°œ í´ë˜ìŠ¤ë§Œ
TARGET_CLASSES = [
    "pushup", "pullup", "squat", "jumpjack",
    "benchpress", "frontraise", "situp"
]

# âœ… CSV ë‚´ ë‹¤ì–‘í•œ í‘œê¸°ë¥¼ í‘œì¤€í™”
LABEL_MAP = {
    "push_up": "pushup",
    "pushups": "pushup",
    "pull_up": "pullup",
    "pullups": "pullup",
    "squant": "squat",
    "squat": "squat",
    "jump_jack": "jumpjack",
    "jumpjacks": "jumpjack",
    "bench_pressing": "benchpress",
    "benchpressing": "benchpress",
    "front_raise": "frontraise",
    "frontraise": "frontraise",
    "situp": "situp",
}

# âœ… ë¬´ì‹œí•  í´ë˜ìŠ¤
IGNORE_CLASSES = ["battle_rope", "pommelhorse", "others"]


def extract_keypoints(video_path, num_frames=64):
    """Mediapipeë¥¼ ì‚¬ìš©í•´ ë¹„ë””ì˜¤ì—ì„œ keypoints ì¶”ì¶œ"""
    cap = cv2.VideoCapture(video_path)
    pose = mp_pose.Pose(static_image_mode=False)
    frames = []
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(1, total // num_frames)

    for i in range(0, total, step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = pose.process(frame)
        if result.pose_landmarks:
            # âœ… visibility í¬í•¨í•˜ì—¬ [33, 4] êµ¬ì¡°ë¡œ ì €ì¥
            pts = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in result.pose_landmarks.landmark])
        else:
            pts = np.zeros((33, 4))  # âœ… ë™ì¼í•œ shape ìœ ì§€
        frames.append(pts)

    cap.release()
    pose.close()

    if len(frames) < num_frames:
        frames += [frames[-1]] * (num_frames - len(frames))
    return np.array(frames[:num_frames])

def process_split(csv_path, video_root, out_root):
    """train/valid/test ê°ê° ì²˜ë¦¬"""
    df = pd.read_csv(csv_path)
    print(f"\nğŸ“‚ {os.path.basename(csv_path)} ì²˜ë¦¬ ì¤‘ ({len(df)}ê°œ)")

    # âœ… ë¼ë²¨ ì •ê·œí™”
    df["type"] = df["type"].str.lower().map(lambda x: LABEL_MAP.get(x, x))

    # âœ… ë¶ˆí•„ìš” í´ë˜ìŠ¤ ì œê±°
    df = df[~df["type"].isin(IGNORE_CLASSES)]

    # âœ… 7ê°œ í´ë˜ìŠ¤ë§Œ ë‚¨ê¸°ê¸°
    df = df[df["type"].isin(TARGET_CLASSES)]

    print(f"â†’ ì‚¬ìš© í´ë˜ìŠ¤: {sorted(df['type'].unique().tolist())}")
    print(f"â†’ ë³€í™˜í•  ì˜ìƒ ìˆ˜: {len(df)}ê°œ\n")

    for _, row in tqdm(df.iterrows(), total=len(df)):
        video_file = row["name"]
        label = row["type"]
        video_path = os.path.join(video_root, video_file)

        if not os.path.exists(video_path):
            print(f"âš ï¸ ì˜ìƒ ì—†ìŒ: {video_path}")
            continue

        # ì €ì¥ ê²½ë¡œ ìƒì„±
        save_dir = os.path.join(out_root, label)
        os.makedirs(save_dir, exist_ok=True)
        npz_path = os.path.join(save_dir, os.path.splitext(video_file)[0] + ".npz")

        # âœ… ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ë°ì´í„° ë³´í˜¸)
        if os.path.exists(npz_path):
            continue

        keypoints = extract_keypoints(video_path)
        np.savez_compressed(npz_path, keypoints=keypoints)

    print(f"âœ… {os.path.basename(csv_path)} ì™„ë£Œ!\n")


if __name__ == "__main__":
    base_dir = r"C:\mycla\TransRAC-main\RepCountA"
    csv_dir = os.path.join(base_dir, "annotation")

    for split in ["train", "valid", "test"]:
        csv_path = os.path.join(csv_dir, f"{split}.csv")
        video_root = os.path.join(base_dir, "video", split)
        out_root = os.path.join(base_dir, "annotation", "skeleton_npz", split)

        process_split(csv_path, video_root, out_root)
