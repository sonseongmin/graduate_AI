import sys, os
sys.stdout.reconfigure(encoding='utf-8')
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import torch
import numpy as np
import cv2
import mediapipe as mp
from models.Transformer_7class_Encoder import HybridLSTMTransformer  # ëª¨ë¸ ì •ì˜ íŒŒì¼
from dataset.RepCountA_Loader import normalize_keypoints


# -----------------------------------------------------
# ğŸ”§ 1. í™˜ê²½ ì„¤ì •
# -----------------------------------------------------
MODEL_PATH = r"C:\mycla\TransRAC-main\models\best_classifier_hybrid_7class.pt"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 7ê°œ í´ë˜ìŠ¤ ì´ë¦„ (ë¼ë²¨ ì¸ë±ìŠ¤ ìˆœì„œ ë™ì¼)
CLASS_NAMES = ["benchpress", "frontraise", "jumpjack", "pullup", "pushup", "situp", "squat"]

mp_pose = mp.solutions.pose


# -----------------------------------------------------
# ğŸ¬ 2. Mediapipeë¡œ ë¹„ë””ì˜¤ â†’ keypoints ì¶”ì¶œ
# -----------------------------------------------------
def extract_keypoints(video_path, num_frames=64):
    cap = cv2.VideoCapture(video_path)
    pose = mp_pose.Pose(static_image_mode=False)
    frames = []
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(1, total // num_frames)

    for i in range(0, total, step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = pose.process(frame)
        if result.pose_landmarks:
            pts = np.array([[lm.x, lm.y, lm.z] for lm in result.pose_landmarks.landmark])
        else:
            pts = np.zeros((33, 3))
        frames.append(pts)

    cap.release()
    pose.close()

    # ë¶€ì¡±í•œ í”„ë ˆì„ ë³´ì •
    if len(frames) < num_frames:
        frames += [frames[-1]] * (num_frames - len(frames))
    frames = np.array(frames[:num_frames])

    # ì •ê·œí™” (mid-hip ì¤‘ì‹¬ + ì–´ê¹¨ ê±°ë¦¬ ìŠ¤ì¼€ì¼)
    frames = normalize_keypoints(frames)

    # Flatten [T, 33, D] â†’ [T, 99]
    frames = frames.reshape(num_frames, -1).astype(np.float32)
    return torch.tensor(frames).unsqueeze(0)  # [1, T, 99]


# -----------------------------------------------------
# ğŸ§  3. ì˜ˆì¸¡ í•¨ìˆ˜
# -----------------------------------------------------
def predict_exercise(video_path):
    # ëª¨ë¸ ë¡œë“œ
    model = HybridLSTMTransformer(
        input_dim=99, hidden_dim=256, num_heads=4, num_layers=2, num_classes=len(CLASS_NAMES)
    ).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()

    # ë¹„ë””ì˜¤ â†’ keypoints
    data = extract_keypoints(video_path).to(DEVICE)

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(data)
        pred_idx = outputs.argmax(dim=1).item()
        confidence = torch.softmax(outputs, dim=1)[0, pred_idx].item()

    print(f"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {CLASS_NAMES[pred_idx]}  ({confidence*100:.2f}% í™•ì‹ )")


# -----------------------------------------------------
# ğŸš€ 4. ì‹¤í–‰ë¶€
# -----------------------------------------------------
if __name__ == "__main__":
    test_video = r"C:\mycla\TransRAC-main\RepCountA\video\test\stu1_27.mp4"  # ğŸ”¸ í…ŒìŠ¤íŠ¸í•  ì˜ìƒ ê²½ë¡œ ìˆ˜ì •
    predict_exercise(test_video)
